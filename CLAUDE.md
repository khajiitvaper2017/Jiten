# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Jiten is a free, open-source platform for Japanese immersion learners that analyses Japanese media to provide detailed statistics (character count, difficulty, vocabulary lists, frequency lists, etc.) and generates Anki decks. See https://jiten.moe for the live platform.

## Repository Structure

```
/
├── Jiten.Api/            # ASP.NET Core Web API
├── Jiten.Cli/            # Command-line tool for batch processing
├── Jiten.Core/           # Core library with domain models and data access
├── Jiten.Parser/         # Japanese text parsing engine
├── Jiten.Tests/          # xUnit test suite
├── Jiten.Web/            # Nuxt 4 frontend application (Vue 4, PrimeVue 4, TailwindCSS 4)
├── Shared/               # Shared resources (dictionaries, ML models, config)
└── Jiten.sln             # Main .NET solution file
```

## Build & Run Commands

```bash
# Backend (.NET 9.0) - run from root directory
dotnet build Jiten.sln
dotnet run --project Jiten.Api/Jiten.Api.csproj          # API at https://localhost:7299
dotnet run --project Jiten.Cli/Jiten.Cli.csproj -- [options]
dotnet test Jiten.Tests/Jiten.Tests.csproj
dotnet test --filter "FullyQualifiedName~DeconjugatorTests.DeconjugationTest"  # Single test

# Frontend - run from Jiten.Web/
pnpm install && pnpm dev    # Dev server at https://localhost:3000
pnpm build                  # Production build
pnpm lint / pnpm lintfix    # Lint
```

## Solution Architecture

**Jiten.Core** - Domain models (`Deck`, `DeckWord`, `JmDictWord`, `ExampleSentence`, etc.), data access (`JitenDbContext` for main data, `UserDbContext` for auth), PostgreSQL + EF Core, metadata providers (Anilist, VNDB, Google Books, IGDB, TMDB).

**Jiten.Parser** - Japanese text parsing engine. MorphologicalAnalyser (Sudachi native interop), Deconjugator (rule-based from `deconjugator.json`), Parser (JMDict lookup pipeline), Redis-backed caching. See [Processing Pipeline](#japanese-text-processing-pipeline) below.

**Jiten.Cli** - Batch processing CLI. Format-specific extractors (KiriKiri, BGI, YuRis, PSB, etc.), bruteforce regex extractor, text/subtitle/manga extractors. Commands for parsing, extraction, metadata download, dictionary import.

**Jiten.Api** - ASP.NET Core Web API. JWT + API Key + Google OAuth authentication. Hangfire background jobs (`ParseJob`, `ComputationJob`, `FetchMetadataJob`, `ReparseJob`). Rate limiting with tiered access. OpenTelemetry observability. Swagger at `/swagger`.

**Jiten.Tests** - xUnit + FluentAssertions. Tests: `DeconjugatorTests`, `MorphologicalAnalyserTests`, `FsrsTests`.

**Jiten.Web** - Nuxt 4 frontend (Vue 4, TypeScript, Pinia). PrimeVue 4 components with TailwindCSS 4. API calls via `useApiFetch` composable with JWT auto-refresh. File-based routing with `auth`/`authAdmin` middleware. Stores: `authStore` (JWT + Google OAuth), `jitenStore` (preferences), `displayStyleStore` (UI). Nuxt auto-imports composables, components, and utils.

### Dependency Flow
- Jiten.Api → Jiten.Core, Jiten.Parser
- Jiten.Cli → Jiten.Core, Jiten.Parser
- Jiten.Parser → Jiten.Core
- Jiten.Tests → Jiten.Parser

## Database Architecture

**JitenDbContext (schema: jiten)** - `Decks` (media entries with stats, parent-child relationships, external links), `DeckWords` (WordId + ReadingIndex + Occurrences), `DeckRawText`, `ExampleSentences`.

**JitenDbContext (schema: jmdict)** - `Words` (JMDict entries with Readings[], PartsOfSpeech[], PitchAccents[]), `Definitions` (multilingual), `Lookups` (text → WordIds), `WordFrequencies`.

**UserDbContext** - ASP.NET Identity, `UserCoverage`, `UserMetadata`, `ApiKeys`, `RefreshTokens`, `UserKnownWord`, `UserDeckPreference`, `UserWordSetState`.

**Key indexes**: PGroonga full-text search on Decks titles, WordId + ReadingIndex composites, DeckId indexes.

### EF Migrations
```bash
dotnet ef database update --project Jiten.Core --startup-project Jiten.Core
dotnet ef migrations add MigrationName --project Jiten.Core --startup-project Jiten.Core --context JitenDbContext
```

## Japanese Text Processing Pipeline

1. **Morphological Analysis** (`Jiten.Parser/MorphologicalAnalyser.cs`): Sudachi tokenises text → WordInfo objects (Text, DictionaryForm, PartOfSpeech, NormalizedForm)
2. **Deconjugation** (`Jiten.Parser/Deconjugator.cs`): Applies rules from `deconjugator.json` → possible base forms with conjugation history
3. **JMDict Lookup** (`Jiten.Parser/Parser.cs`): Queries Lookups table → matches by POS compatibility → priority scoring for ambiguous matches
4. **Result** (`Jiten.Core/Data/DeckWord.cs`): WordId, ReadingIndex, OriginalText, Conjugations, Occurrences

**Caching**: Redis-backed word cache by (Text, POS, DictionaryForm) tuples. JMDict cache populated in 10K batches. Cache failures are non-fatal. **You MUST flush Redis after parser changes** (`dotnet run --project Jiten.Cli -- --flush-redis`).

## Autonomous Parser Testing

**Test commands:**
```bash
dotnet run --project Jiten.Cli -- --parse-test "飲んだから"                    # Single input diagnostics
dotnet run --project Jiten.Cli -- --parse-test "食べている" --parse-test-output diag.json
dotnet run --project Jiten.Cli -- --run-parser-tests                          # Batch tests
dotnet run --project Jiten.Cli -- --run-parser-tests --parse-test-output failures.json
dotnet run --project Jiten.Cli -- --deconjugate-test "飲んだ"                   # Show all deconjugation forms
dotnet run --project Jiten.Cli -- --deconjugate-test "食べさせられた" --parse-test-output deconj.json
```

**Database search (for diagnostics):**
```bash
dotnet run --project Jiten.Cli -- --search-word 2084700        # By WordId
dotnet run --project Jiten.Cli -- --search-word "そうする"      # By reading
dotnet run --project Jiten.Cli -- --search-lookup "そうする"    # Lookups table
```

**Diagnostic JSON output contains**: `sudachi.tokens` (raw Sudachi analysis), `sudachi.rawOutput`, `tokenStages` (processing stages with modifications), `results` (final parsed tokens).

**Failure types:**
1. **OverSegmentation** - Tokens split that should be combined → check `sudachi.tokens` and `tokenStages` → fix: add to `SpecialCases2`/`SpecialCases3`
2. **UnderSegmentation** - Tokens merged that should be separate → check for `"type": "merge"` in modifications → fix: add exclusion in Combine* method or `PreprocessText()`
3. **TokenMismatch** - Content differs → usually POS misclassification or wrong dictionary form

**Key files for parser fixes:**

- `Jiten.Parser/MorphologicalAnalyser.cs`: `SpecialCases2`/`SpecialCases3` (hardcoded token combinations), `MisparsesRemove` (tokens to filter), `Combine*` methods (merging logic), `PreprocessText()` (forced splits), `RepairNTokenisation()` (ん-form fixes)
- `Shared/resources/deconjugator.json`: Rule-based deconjugation (~1500 rules). Types: `stdrule`, `rewriterule`, `onlyfinalrule`, `neverfinalrule`, `contextrule`. Fields: `dec_end`/`con_end` (endings), `dec_tag`/`con_tag` (grammar tags). Search for specific endings or `"detail": "past"` etc.
- `Shared/resources/user_dic.xml`: Custom Sudachi dictionary entries. Format: `surface,leftId,rightId,cost,display,pos1,pos2,pos3,pos4,conjType,conjForm,reading,normalised,dictFormId,splitType,splitA,splitB,unused`. Regenerate after editing: `sudachi ubuild "Y:\CODE\Jiten\Shared\resources\user_dic.xml" -s "S:\Jiten\sudachi.rs\resources\system_full.dic" -o "Y:\CODE\Jiten\Shared\resources\user_dic.dic"`

**Autonomous fix workflow:**
1. `--run-parser-tests` → identify failures
2. `--parse-test "input"` → full diagnostics per failure
3. Analyse `sudachi` and `tokenStages` to identify cause
4. Apply fix: Sudachi issue → `user_dic.xml`/`PreprocessText()`; missing combination → `SpecialCases2/3`; wrong merge → Combine* method; deconjugation → `deconjugator.json`; word matching → `FindValidCompoundWordId`/`GetBestReadingIndex`
5. **Flush Redis** with `--flush-redis`
6. Re-run failing test, then full suite for regressions

## Shared Resources

`Shared/` is copied to output directories at build time. Contains: Sudachi config and native binaries (`sudachi_lib.dll`/`libsudachi_lib.so`), custom user dictionary (`user_dic.dic`/`user_dic.xml`), `deconjugator.json`, ONNX difficulty models (novels + shows), Anki template (`lapis.apkg`), `sharedsettings.json`.

## Configuration

Layered: `Shared/sharedsettings.json` → `appsettings.json` → `appsettings.{Environment}.json` → env vars. Required: PostgreSQL, Redis, JWT secret, SMTP, BunnyCDN, Google OAuth. See `sharedsettings.example.json`.
